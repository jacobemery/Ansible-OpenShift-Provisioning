<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://ibm.github.io/Ansible-OpenShift-Provisioning/set-variables-group-vars/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>2 Set Variables (group_vars) - Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2 Set Variables (group_vars)";
        var mkdocs_page_input_path = "set-variables-group-vars.md";
        var mkdocs_page_url = "/Ansible-OpenShift-Provisioning/set-variables-group-vars/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="..">
          <img src="../images/ansible-logo.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Read Me</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../before-you-begin/">Before You Begin</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../prerequisites/">Prerequisites</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Installation Instructions</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../get-info/">1 Get Info</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">2 Set Variables (group_vars)</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#1-controller">1 - Controller</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-lpars">2 - LPAR(s)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-file-server">3 - File Server</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-red-hat-info">4 - Red Hat Info</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#5-bastion">5 - Bastion</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-cluster-networking">6 - Cluster Networking</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#7-bootstrap-node">7 - Bootstrap Node</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#8-control-nodes">8 - Control Nodes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#9-compute-nodes">9 - Compute Nodes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#10-infra-nodes">10 - Infra Nodes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#11-optional-packages">11 - (Optional) Packages</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#12-openshift-settings">12 - OpenShift Settings</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#13-optional-proxy">13 - (Optional) Proxy</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#14-optional-misc">14 - (Optional) Misc</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#15-ocp-and-rhcos-coreos">15 - OCP and RHCOS (CoreOS)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#16-hypershift-optional">16 - Hypershift ( Optional )</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#17-optional-create-compute-node-in-a-day-2-operation">17 - (Optional) Create compute node in a day-2 operation</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../set-variables-host-vars/">3 Set Variables (host_vars)</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run-the-playbooks/">4 Run the Playbooks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run-the-playbooks-for-hypershift/">Run the Playbooks (HyperShift)</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Misc</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../acknowledgements/">Acknowledgements</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Ansible-Automated OpenShift Provisioning on KVM on IBM zSystems / LinuxONE</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
          <li>Installation Instructions &raquo;</li>
      <li class="breadcrumb-item active">2 Set Variables (group_vars)</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/edit/main/docs/set-variables-group-vars.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="step-2-set-variables-group_vars">Step 2: Set Variables (group_vars)<a class="headerlink" href="#step-2-set-variables-group_vars" title="Permanent link">#</a></h1>
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">#</a></h2>
<ul>
<li>In a text editor of your choice, open the template of the <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/blob/main/inventories/default/group_vars/all.yaml.template">environment variables file</a>. Make a copy of it called all.yaml and paste it into the same directory with its template.</li>
<li>all.yaml is your master variables file and you will likely reference it many times throughout the process. The default inventory can be found at <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning/blob/main/inventories/default">inventories/default</a>.</li>
<li>The variables marked with an <code>X</code> are required to be filled in. Many values are pre-filled or are optional. Optional values are commented out; in order to use them, remove the <code>#</code> and fill them in.</li>
<li>This is the most important step in the process. Take the time to make sure everything here is correct.</li>
<li><u>Note on YAML syntax</u>: Only the lowest value in each hierarchicy needs to be filled in. For example, at the top of the variables file env and z don't need to be filled in, but the cpc_name does. There are X's where input is required to help you with this.</li>
<li>Scroll the table to the right to see examples for each variable.</li>
</ul>
<h2 id="1-controller">1 - Controller<a class="headerlink" href="#1-controller" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.controller.sudo_pass</strong></td>
<td style="text-align: left;">The password to the machine running Ansible (localhost).<br /> This will only be used for two things. To ensure you've installed the<br /> pre-requisite packages if you're on Linux, and to add the login URL<br /> to your /etc/hosts file.</td>
<td style="text-align: left;">Pas$w0rd!</td>
</tr>
</tbody>
</table>
<h2 id="2-lpars">2 - LPAR(s)<a class="headerlink" href="#2-lpars" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.z.high_availability</strong></td>
<td style="text-align: left;">Is this cluster spread across three LPARs? If yes, mark True. If not (just in<br /> one LPAR), mark False</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.ip_forward</strong></td>
<td style="text-align: left;">This variable specifies if ip forwarding is enabled or not if NAT network is selected. If ip_forwarding is set to 0, the installed OCP cluster will not be able to access external services. This setting will be configured during 3_setup_kvm playbook. If NAT will be configured after 3_setup_kvm playbook, the setup needs to be done manually before bastion is being created, configured or reconfigured by running the 3_setup_kvm playbook with parameter: --tags cfg_ip_forward</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar1.create</strong></td>
<td style="text-align: left;">To have Ansible create an LPAR and install RHEL on it for the KVM<br /> host, mark True. If using a pre-existing LPAR with RHEL already<br /> installed, mark False.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar1.hostname</strong></td>
<td style="text-align: left;">The hostname of the KVM host.</td>
<td style="text-align: left;">kvm-host-01</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar1.ip</strong></td>
<td style="text-align: left;">The IPv4 address of the KVM host.</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar1.user</strong></td>
<td style="text-align: left;">Username for Linux admin on KVM host 1. Recommended to run as a non-root user with sudo access.</td>
<td style="text-align: left;">admin</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar1.pass</strong></td>
<td style="text-align: left;">The password for the user that will be created or exists on the KVM host.</td>
<td style="text-align: left;">ch4ngeMe!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar2.create</strong></td>
<td style="text-align: left;">To create a second LPAR and install RHEL on it to act as<br /> another KVM host, mark True. If using pre-existing LPAR(s) with RHEL<br /> already installed, mark False.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar2.hostname</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The hostname of the second KVM host.</td>
<td style="text-align: left;">kvm-host-02</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar2.ip</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The IPv4 address of the second KVM host.</td>
<td style="text-align: left;">192.168.10.2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar2.user</strong></td>
<td style="text-align: left;">Username for Linux admin on KVM host 2. Recommended to run as a non-root user with sudo access.</td>
<td style="text-align: left;">admin</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar2.pass</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The password for the admin user on the second KVM host.</td>
<td style="text-align: left;">ch4ngeMe!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar3.create</strong></td>
<td style="text-align: left;">To create a third LPAR and install RHEL on it to act as<br /> another KVM host, mark True. If using pre-existing LPAR(s) with RHEL<br /> already installed, mark False.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar3.hostname</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The hostname of the third KVM host.</td>
<td style="text-align: left;">kvm-host-03</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar3.ip</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The IPv4 address of the third KVM host.</td>
<td style="text-align: left;">192.168.10.3</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar3.user</strong></td>
<td style="text-align: left;">Username for Linux admin on KVM host 3. Recommended to run as a non-root user with sudo access.</td>
<td style="text-align: left;">admin</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.z.lpar3.pass</strong></td>
<td style="text-align: left;"><b>(Optional)</b> The password for the admin user on the third KVM host.</td>
<td style="text-align: left;">ch4ngeMe!</td>
</tr>
</tbody>
</table>
<h2 id="3-file-server">3 - File Server<a class="headerlink" href="#3-file-server" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.file_server.ip</strong></td>
<td style="text-align: left;">IPv4 address for the file server that will be used to pass config files and<br /> iso to KVM host LPAR(s) and bastion VM during their first boot.</td>
<td style="text-align: left;">192.168.10.201</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.file_server.user</strong></td>
<td style="text-align: left;">Username to connect to the file server. Must have sudo and SSH access.</td>
<td style="text-align: left;">user1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.file_server.pass</strong></td>
<td style="text-align: left;">Password to connect to the file server as above user.</td>
<td style="text-align: left;">user1pa$s!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.file_server.protocol</strong></td>
<td style="text-align: left;">Protocol used to serve the files, either 'ftp' or 'http'</td>
<td style="text-align: left;">http</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.file_server.iso_mount_dir</strong></td>
<td style="text-align: left;">Directory path relative to the HTTP/FTP accessible directory where RHEL ISO is mounted. For example, if the FTP root is at /home/user1<br /> and the ISO is mounted at /home/user1/RHEL/8.7 then this variable would be<br /> RHEL/8.7 - no slash before or after.</td>
<td style="text-align: left;">RHEL/8.7</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.file_server.cfgs_dir</strong></td>
<td style="text-align: left;">Directory path relative to to the HTTP/FTP accessible directory where configuration files can be stored. For example, if FTP root is /home/user1<br /> and you would like to store the configs at /home/user1/ocpz-config then this variable would be<br /> ocpz-config. No slash before or after.</td>
<td style="text-align: left;">ocpz-config</td>
</tr>
</tbody>
</table>
<h2 id="4-red-hat-info">4 - Red Hat Info<a class="headerlink" href="#4-red-hat-info" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.redhat.username</strong></td>
<td style="text-align: left;">Red Hat username with a valid license or free trial to Red Hat<br /> OpenShift Container Platform (RHOCP), which comes with<br /> necessary licenses for Red Hat Enterprise Linux (RHEL) and<br /> Red Hat CoreOS (RHCOS).</td>
<td style="text-align: left;">redhat.user</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.redhat.password</strong></td>
<td style="text-align: left;">Password to Red Hat above user's account. Used to auto-attach<br /> necessary subscriptions to KVM Host, bastion VM, and pull live<br /> images for OpenShift.</td>
<td style="text-align: left;">rEdHatPa$s!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.redhat.pull_secret</strong></td>
<td style="text-align: left;">Pull secret for OpenShift, comes from Red Hat's <a href="https://console.redhat.com/openshift/install/ibmz/user-provisioned">Hybrid Cloud Console</a>.<br /> Make sure to enclose in 'single quotes'.<br /></td>
<td style="text-align: left;">'{"auths":{"cloud.openshift<br />.com":{"auth":"b3Blb<br />...<br />4yQQ==","email":"redhat.<br />user@gmail.com"}}}'</td>
</tr>
</tbody>
</table>
<h2 id="5-bastion">5 - Bastion<a class="headerlink" href="#5-bastion" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.bastion.create</strong></td>
<td style="text-align: left;">True or False. Would you like to create a bastion KVM guest to host essential infrastructure services like DNS,<br /> load balancer, firewall, etc? Can de-select certain services with the env.bastion.options<br /> variables below.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.vm_name</strong></td>
<td style="text-align: left;">Name of the bastion VM. Arbitrary value.</td>
<td style="text-align: left;">bastion</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.resources.disk_size</strong></td>
<td style="text-align: left;">How much of the storage pool would you like to allocate to the bastion (in<br /> Gigabytes)? Recommended 30 or more.</td>
<td style="text-align: left;">30</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.resources.ram</strong></td>
<td style="text-align: left;">How much memory would you like to allocate the bastion (in<br /> megabytes)? Recommended 4096 or more</td>
<td style="text-align: left;">4096</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.resources.swap</strong></td>
<td style="text-align: left;">How much swap storage would you like to allocate the bastion (in<br /> megabytes)? Recommended 4096 or more.</td>
<td style="text-align: left;">4096</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.resources.vcpu</strong></td>
<td style="text-align: left;">How many virtual CPUs would you like to allocate to the bastion? Recommended 4 or more.</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.ip</strong></td>
<td style="text-align: left;">IPv4 address for the bastion.</td>
<td style="text-align: left;">192.168.10.3</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.hostname</strong></td>
<td style="text-align: left;">Hostname of the bastion. Will be combined with<br /> env.bastion.networking.base_domain to create a Fully Qualified Domain Name (FQDN).</td>
<td style="text-align: left;">ocpz-bastion</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.base_<br />domain</strong></td>
<td style="text-align: left;">Base domain that, when combined with the hostname, creates a fully-qualified<br /> domain name (FQDN) for the bastion?</td>
<td style="text-align: left;">ihost.com</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.<br />subnetmask</strong></td>
<td style="text-align: left;">Subnet of the bastion.</td>
<td style="text-align: left;">255.255.255.0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.gateway</strong></td>
<td style="text-align: left;">IPv4 of he bastion's gateway server.</td>
<td style="text-align: left;">192.168.10.0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.name<br />server1</strong></td>
<td style="text-align: left;">IPv4 address of the server that resolves the bastion's hostname.</td>
<td style="text-align: left;">192.168.10.200</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.name<br />server2</strong></td>
<td style="text-align: left;"><b>(Optional)</b> A second IPv4 address that resolves the bastion's hostname.</td>
<td style="text-align: left;">192.168.10.201</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.forwarder</strong></td>
<td style="text-align: left;">What IPv4 address will be used to make external DNS calls for the bastion? Can use 1.1.1.1 or 8.8.8.8 as defaults.</td>
<td style="text-align: left;">8.8.8.8</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.networking.interface</strong></td>
<td style="text-align: left;">Name of the networking interface on the bastion from Linux's perspective. Most likely enc1.</td>
<td style="text-align: left;">enc1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.access.user</strong></td>
<td style="text-align: left;">What would you like the admin's username to be on the bastion?<br /> If root, make pass and root_pass vars the same.</td>
<td style="text-align: left;">admin</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.access.pass</strong></td>
<td style="text-align: left;">The password to the bastion's admin user. If using root, make<br /> pass and root_pass vars the same.</td>
<td style="text-align: left;">cH4ngeM3!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.access.root_pass</strong></td>
<td style="text-align: left;">The root password for the bastion. If using root, make<br /> pass and root_pass vars the same.</td>
<td style="text-align: left;">R0OtPa$s!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.options.dns</strong></td>
<td style="text-align: left;">Would you like the bastion to host the DNS information for the<br /> cluster? True or False. If false, resolution must come from<br /> elsewhere in your environment. Make sure to add IP addresses for<br /> KVM hosts, bastion, bootstrap, control, compute nodes, AND api,<br /> api-int and *.apps as described <a href="https://docs.openshift.com/container-platform/4.8/installing/installing_bare_metal/installing-bare-metal-network-customizations.html">here</a> in section "User-provisioned<br /> DNS Requirements" Table 5. If True this will be done for you in<br /> the dns and check_dns roles.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.options.load<br />balancer.on_bastion</strong></td>
<td style="text-align: left;">Would you like the bastion to host the load balancer (HAProxy) for the cluster?<br /> True or False (boolean).<br /> If false, this service must be provided elsewhere in your environment, and public and<br /> private IP of the load balancer must be<br /> provided in the following two variables.</td>
<td style="text-align: left;">True</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.options.load<br />balancer.public_ip</strong></td>
<td style="text-align: left;">(Only required if env.bastion.options.loadbalancer.on_bastion is True). The public IPv4<br /> address for your environment's loadbalancer. api, apps, *.apps must use this.</td>
<td style="text-align: left;">192.168.10.50</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bastion.options.load<br />balancer.private_ip</strong></td>
<td style="text-align: left;">(Only required if env.bastion.options.loadbalancer.on_bastion is True). The private IPv4 address<br /> for your environment's loadbalancer. api-int must use this.</td>
<td style="text-align: left;">10.24.17.12</td>
</tr>
</tbody>
</table>
<h2 id="6-cluster-networking">6 - Cluster Networking<a class="headerlink" href="#6-cluster-networking" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.cluster.networking.metadata_name</strong></td>
<td style="text-align: left;">Name to describe the cluster as a whole, can be anything if DNS will be hosted on the bastion. If<br /> DNS is not on the bastion, must match your DNS configuration. Will be combined with the base_domain<br /> and hostnames to create Fully Qualified Domain Names (FQDN).</td>
<td style="text-align: left;">ocpz</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.networking.base_domain</strong></td>
<td style="text-align: left;">The site name, where is the cluster being hosted? This will be combined with the metadata_name<br /> and hostnames to create FQDNs.</td>
<td style="text-align: left;">ihost.com</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.networking.nameserver1</strong></td>
<td style="text-align: left;">IPv4 address that the cluster get its hostname resolution from. If env.bastion.options.dns<br /> is True, this should be the IP address of the bastion.</td>
<td style="text-align: left;">192.168.10.200</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.networking.nameserver2</strong></td>
<td style="text-align: left;"><b>(Optional)</b> A second IPv4 address will the cluster get its hostname resolution from? If env.bastion.options.dns<br /> is True, this should be left commented out.</td>
<td style="text-align: left;">192.168.10.201</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.networking.forwarder</strong></td>
<td style="text-align: left;">What IPv4 address will be used to make external DNS calls for the cluster? Can use 1.1.1.1 or 8.8.8.8 as defaults.</td>
<td style="text-align: left;">8.8.8.8</td>
</tr>
</tbody>
</table>
<h2 id="7-bootstrap-node">7 - Bootstrap Node<a class="headerlink" href="#7-bootstrap-node" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.disk_size</strong></td>
<td style="text-align: left;">How much disk space do you want to allocate to the bootstrap node (in Gigabytes)? Bootstrap node<br /> is temporary and will be brought down automatically when its job completes. 120 or more recommended.</td>
<td style="text-align: left;">120</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.ram</strong></td>
<td style="text-align: left;">How much memory would you like to allocate to the temporary bootstrap node (in<br /> megabytes)? Recommended 16384 or more.</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.vcpu</strong></td>
<td style="text-align: left;">How many virtual CPUs would you like to allocate to the temporary bootstrap node?<br /> Recommended 4 or more.</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.vm_name</strong></td>
<td style="text-align: left;">Name of the temporary bootstrap node VM. Arbitrary value.</td>
<td style="text-align: left;">bootstrap</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.ip</strong></td>
<td style="text-align: left;">IPv4 address of the temporary bootstrap node.</td>
<td style="text-align: left;">192.168.10.4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.bootstrap.hostname</strong></td>
<td style="text-align: left;">Hostname of the temporary boostrap node. If DNS is hosted on the bastion, this can be anything.<br /> If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the<br /> metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td style="text-align: left;">bootstrap-ocpz</td>
</tr>
</tbody>
</table>
<h2 id="8-control-nodes">8 - Control Nodes<a class="headerlink" href="#8-control-nodes" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.disk_size</strong></td>
<td style="text-align: left;">How much disk space do you want to allocate to each control node (in Gigabytes)? 120 or more recommended.</td>
<td style="text-align: left;">120</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.ram</strong></td>
<td style="text-align: left;">How much memory would you like to allocate to the each control<br /> node (in megabytes)? Recommended 16384 or more.</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.vcpu</strong></td>
<td style="text-align: left;">How many virtual CPUs would you like to allocate to each control node? Recommended 4 or more.</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.vm_name</strong></td>
<td style="text-align: left;">Name of the control node VMs. Arbitrary values. Usually no more or less than 3 are used. Must match<br /> the total number of IP addresses and hostnames for control nodes. Use provided list format.</td>
<td style="text-align: left;">control-1<br />control-2<br />control-3</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.ip</strong></td>
<td style="text-align: left;">IPv4 address of the control nodes. Use provided<br /> list formatting.</td>
<td style="text-align: left;">192.168.10.5<br />192.168.10.6<br />192.168.10.7</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.control.hostname</strong></td>
<td style="text-align: left;">Hostnames for control nodes. Must match the total number of IP addresses for control nodes<br /> (usually 3). If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere,<br /> this must match DNS definition. This will be combined with the metadata_name and<br /> base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td style="text-align: left;">control-01<br />control-02<br />control-03</td>
</tr>
</tbody>
</table>
<h2 id="9-compute-nodes">9 - Compute Nodes<a class="headerlink" href="#9-compute-nodes" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.disk_size</strong></td>
<td style="text-align: left;">How much disk space do you want to allocate to each compute<br /> node (in Gigabytes)? 120 or more recommended.</td>
<td style="text-align: left;">120</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.ram</strong></td>
<td style="text-align: left;">How much memory would you like to allocate to the each compute<br /> node (in megabytes)? Recommended 16384 or more.</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.vcpu</strong></td>
<td style="text-align: left;">How many virtual CPUs would you like to allocate to each compute node? Recommended 2 or more.</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.vm_name</strong></td>
<td style="text-align: left;">Name of the compute node VMs. Arbitrary values. This list can be expanded to any<br /> number of nodes, minimum 2. Must match the total number of IP<br /> addresses and hostnames for compute nodes. Use provided list format.</td>
<td style="text-align: left;">compute-1<br />compute-2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.ip</strong></td>
<td style="text-align: left;">IPv4 address of the compute nodes. Must match the total number of VM names and<br /> hostnames for compute nodes. Use provided list formatting.</td>
<td style="text-align: left;">192.168.10.8<br />192.168.10.9</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.compute.hostname</strong></td>
<td style="text-align: left;">Hostnames for compute nodes. Must match the total number of IP addresses and<br /> VM names for compute nodes. If DNS is hosted on the bastion, this can be anything.<br /> If DNS is hosted elsewhere, this must match DNS definition. This will be combined with the<br /> metadata_name and base_domain to create a Fully Qualififed Domain Name (FQDN).</td>
<td style="text-align: left;">compute-01<br />compute-02</td>
</tr>
</tbody>
</table>
<h2 id="10-infra-nodes">10 - Infra Nodes<a class="headerlink" href="#10-infra-nodes" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.disk_size</strong></td>
<td style="text-align: left;"><b>(Optional)</b> Set up compute nodes that are made for infrastructure workloads (ingress,<br /> monitoring, logging)? How much disk space do you want to allocate to each infra node (in Gigabytes)?<br /> 120 or more recommended.</td>
<td style="text-align: left;">120</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.ram</strong></td>
<td style="text-align: left;"><b>(Optional)</b> How much memory would you like to allocate to the each infra node (in<br /> megabytes)? Recommended 16384 or more.</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.vcpu</strong></td>
<td style="text-align: left;"><b>(Optional)</b> How many virtual CPUs would you like to allocate to each infra node?<br /> Recommended 2 or more.</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.vm_name</strong></td>
<td style="text-align: left;"><b>(Optional)</b> Name of additional infra node VMs. Arbitrary values. This list can be<br /> expanded to any number of nodes, minimum 2. Must match the total<br /> number of IP addresses and hostnames for infra nodes. Use provided list format.</td>
<td style="text-align: left;">infra-1<br />infra-2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.ip</strong></td>
<td style="text-align: left;"><b>(Optional)</b> IPv4 address of the infra nodes. This list can be expanded to any number of nodes,<br /> minimum 2. Use provided list formatting.</td>
<td style="text-align: left;">192.168.10.8<br />192.168.10.9</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.cluster.nodes.infra.hostname</strong></td>
<td style="text-align: left;"><b>(Optional)</b> Hostnames for infra nodes. Must match the total number of IP addresses for infra nodes.<br /> If DNS is hosted on the bastion, this can be anything. If DNS is hosted elsewhere, this must match<br /> DNS definition. This will be combined with the metadata_name and base_domain<br /> to create a Fully Qualififed Domain Name (FQDN).</td>
<td style="text-align: left;">infra-01<br />infra-02</td>
</tr>
</tbody>
</table>
<h2 id="11-optional-packages">11 - (Optional) Packages<a class="headerlink" href="#11-optional-packages" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.pkgs.galaxy</strong></td>
<td style="text-align: left;">A list of Ansible Galaxy collections that will be installed during the setup playbook. The<br /> collections listed are required. Feel free to add more as needed, just make sure to follow the same list format.</td>
<td style="text-align: left;">community.general</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.pkgs.controller</strong></td>
<td style="text-align: left;">A list of packages that will be installed on the machine running Ansible during the setup<br /> playbook. Feel free to add more as needed, just make sure to follow the same list format.</td>
<td style="text-align: left;">openssh</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.pkgs.kvm</strong></td>
<td style="text-align: left;">A list of packages that will be installed on the KVM Host during the setup_kvm_host playbook.<br /> Feel free to add more as needed, just make sure to follow the same list format.</td>
<td style="text-align: left;">qemu-kvm</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.pkgs.bastion</strong></td>
<td style="text-align: left;">A list of packages that will be installed on the bastion during the setup_bastion playbook.<br /> Feel free to add more as needed, just make sure to follow the same list format.</td>
<td style="text-align: left;">haproxy</td>
</tr>
</tbody>
</table>
<h2 id="12-openshift-settings">12 - OpenShift Settings<a class="headerlink" href="#12-openshift-settings" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.install_config.api_version</strong></td>
<td style="text-align: left;">Kubernetes API version for the cluster. These install_config variables will be passed to the OCP<br /> install_config file. This file is templated in the get_ocp role during the setup_bastion playbook.<br /> To make more fine-tuned adjustments to the install_config, you can find it at<br /> roles/get_ocp/templates/install-config.yaml.j2</td>
<td style="text-align: left;">v1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.compute.architecture</strong></td>
<td style="text-align: left;">Computing architecture for the compute nodes. Must be s390x for clusters on IBM zSystems.</td>
<td style="text-align: left;">s390x</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.compute.hyperthreading</strong></td>
<td style="text-align: left;">Enable or disable hyperthreading on compute nodes. Recommended enabled.</td>
<td style="text-align: left;">Enabled</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.control.architecture</strong></td>
<td style="text-align: left;">Computing architecture for the control nodes. Must be s390x for clusters on IBM zSystems.</td>
<td style="text-align: left;">s390x</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.control.hyperthreading</strong></td>
<td style="text-align: left;">Enable or disable hyperthreading on control nodes. Recommended enabled.</td>
<td style="text-align: left;">Enabled</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.cluster_network.cidr</strong></td>
<td style="text-align: left;">IPv4 block in Internal cluster networking in Classless Inter-Domain<br /> Routing (CIDR) notation. Recommended to keep as is.</td>
<td style="text-align: left;">10.128.0.0/14</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.cluster_network.host_prefix</strong></td>
<td style="text-align: left;">The subnet prefix length to assign to each individual node. For example, if<br /> hostPrefix is set to 23 then each node is assigned a /23 subnet out of the given cidr. A hostPrefix<br /> value of 23 provides 510 (2^(32 - 23) - 2) pod IP addresses.</td>
<td style="text-align: left;">23</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.cluster_network.type</strong></td>
<td style="text-align: left;">The cluster network provider Container Network Interface (CNI) plug-in to install.<br /> Either OpenShiftSDN or OVNKubernetes (default).</td>
<td style="text-align: left;">OVNKubernetes</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.service_network</strong></td>
<td style="text-align: left;">The IP address block for services. The default value is 172.30.0.0/16. The OpenShift SDN<br /> and OVN-Kubernetes network providers support only a single IP address block for the service<br /> network. An array with an IP address block in CIDR format.</td>
<td style="text-align: left;">172.30.0.0/16</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.install_config.fips</strong></td>
<td style="text-align: left;">True or False (boolean) for whether or not to use the United States' Federal Information Processing<br /> Standards (FIPS). Not yet certified on IBM zSystems. Enclosed in 'single quotes'.</td>
<td style="text-align: left;">'false'</td>
</tr>
</tbody>
</table>
<h2 id="13-optional-proxy">13 - (Optional) Proxy<a class="headerlink" href="#13-optional-proxy" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.proxy.http</strong></td>
<td style="text-align: left;">(Optional) A proxy URL to use for creating HTTP connections outside the cluster. Will be<br /> used in the install-config and applied to other Ansible hosts unless set otherwise in<br /> no_proxy below. Must follow this pattern: http://username:pswd&gt;@ip:port</td>
<td style="text-align: left;">http://ocp-admin:Pa$sw0rd@9.72.10.1:80</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.proxy.https</strong></td>
<td style="text-align: left;">(Optional) A proxy URL to use for creating HTTPS connections outside the cluster. Will be<br /> used in the install-config and applied to other Ansible hosts unless set otherwise in<br /> no_proxy below. Must follow this pattern: https://username:pswd@ip:port</td>
<td style="text-align: left;">https://ocp-admin:Pa$sw0rd@9.72.10.1:80</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.proxy.no</strong></td>
<td style="text-align: left;">(Optional) A comma-separated list (no spaces) of destination domain names, IP<br /> addresses, or other network CIDRs to exclude from proxying. When using a<br /> proxy, all necessary IPs and domains for your cluster will be added automatically. See<br /> roles/get_ocp/templates/install-config.yaml.j2 for more details on the template. <br />Preface a domain with . to match subdomains only. For example, .y.com matches<br /> x.y.com, but not y.com. Use * to bypass the proxy for all listed destinations.</td>
<td style="text-align: left;">example.com,192.168.10.1</td>
</tr>
</tbody>
</table>
<h2 id="14-optional-misc">14 - (Optional) Misc<a class="headerlink" href="#14-optional-misc" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>env.language</strong></td>
<td style="text-align: left;">What language would you like Red Hat Enterprise Linux to use? In UTF-8 language code.<br /> Available languages and their corresponding codes can be found <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html-single/international_language_support_guide/index">here</a>, in the "Locale" column of Table 2.1.</td>
<td style="text-align: left;">en_US.UTF-8</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.timezone</strong></td>
<td style="text-align: left;">Which timezone would you like Red Hat Enterprise Linux to use? A list of available timezone<br /> options can be found <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">here</a>.</td>
<td style="text-align: left;">America/New_York</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.keyboard</strong></td>
<td style="text-align: left;">Which keyboard layout would you like Red Hat Enterprise Linux to use?</td>
<td style="text-align: left;">us</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.ansible_key_name</strong></td>
<td style="text-align: left;">(Optional) Name of the SSH key that Ansible will use to connect to hosts.</td>
<td style="text-align: left;">ansible-ocpz</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.ocp_key_name</strong></td>
<td style="text-align: left;">Comment to describe the SSH key used for OCP. Arbitrary value.</td>
<td style="text-align: left;">OCPZ-01 key</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.bridge_name</strong></td>
<td style="text-align: left;">(Optional) Name of the macvtap bridge that will be created on the KVM host or in case of NAT the name of the NAT network defenition (usually it is 'default'). If NAT is being used and a jumphost is needed, the parameters network_mode, jumphost.name, jumphost.user and jumphost.pass must be specified, too. In case of default (NAT) network verify that the configured IP ranges does not interfere with the IPs defined for the controle and compute nodes. Modify the default network (dhcp range setting) to prevent issues with VMs using dhcp and OCP nodes having fixed IPs.</td>
<td style="text-align: left;">macvtap-net</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.network_mode</strong></td>
<td style="text-align: left;">(Optional) In case the network mode will be NAT and the installation will be executed from remote (e.g. your laptop), a jumphost needs to be defined to let the installation access the bastion host. If macvtap for networking is being used this variable should be empty.</td>
<td style="text-align: left;">NAT</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.jumphost.name</strong></td>
<td style="text-align: left;">(Optional) If env.network.mode is set to 'NAT' the name of the jumphost (e.g. the name of KVM host if used as jumphost) should be specified.</td>
<td style="text-align: left;">kvm-host-01</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.jumphost.ip</strong></td>
<td style="text-align: left;">(Optional) The ip of the jumphost.</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.jumphost.user</strong></td>
<td style="text-align: left;">(Optional) The user name to login to the jumphost.</td>
<td style="text-align: left;">admin</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.jumphost.pass</strong></td>
<td style="text-align: left;">(Optional) The password for user to login to the jumphost.</td>
<td style="text-align: left;">ch4ngeMe!</td>
</tr>
<tr>
<td style="text-align: left;"><strong>env.jumphost.path_to_keypair</strong></td>
<td style="text-align: left;">(Optional) The absolute path to the public key file on the jumphost to be copied to the bastion.</td>
<td style="text-align: left;">/home/admin/.ssh/id_rsa.pub</td>
</tr>
</tbody>
</table>
<h2 id="15-ocp-and-rhcos-coreos">15 - OCP and RHCOS (CoreOS)<a class="headerlink" href="#15-ocp-and-rhcos-coreos" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>ocp_download_url</strong></td>
<td style="text-align: left;">Link to the mirror for the OpenShift client and installer from Red Hat.</td>
<td style="text-align: left;">https://mirror.openshift.com<br />/pub/openshift-v4/multi<br />/clients/ocp/4.13.1/s390x/</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ocp_client_tgz</strong></td>
<td style="text-align: left;">OpenShift client filename (tar.gz).</td>
<td style="text-align: left;">openshift-client-linux.tar.gz</td>
</tr>
<tr>
<td style="text-align: left;"><strong>ocp_install_tgz</strong></td>
<td style="text-align: left;">OpenShift installer filename (tar.gz).</td>
<td style="text-align: left;">openshift-install-linux.tar.gz</td>
</tr>
<tr>
<td style="text-align: left;"><strong>rhcos_download_url</strong></td>
<td style="text-align: left;">Link to the CoreOS files to be used for the bootstrap, control and compute nodes.<br /> Feel free to change to a different version.</td>
<td style="text-align: left;">https://mirror.openshift.com<br />/pub/openshift-v4/s390x<br />/dependencies/rhcos<br />/4.12/4.12.3/</td>
</tr>
<tr>
<td style="text-align: left;"><strong>rhcos_os_variant</strong></td>
<td style="text-align: left;">CoreOS base OS. Use the OS string as defined in 'osinfo-query os -f short-id'</td>
<td style="text-align: left;">rhel8.6</td>
</tr>
<tr>
<td style="text-align: left;"><strong>rhcos_live_kernel</strong></td>
<td style="text-align: left;">CoreOS kernel filename to be used for the bootstrap, control and compute nodes.</td>
<td style="text-align: left;">rhcos-4.12.3-s390x-live-kernel-s390x</td>
</tr>
<tr>
<td style="text-align: left;"><strong>rhcos_live_initrd</strong></td>
<td style="text-align: left;">CoreOS initramfs to be used for the bootstrap, control and compute nodes.</td>
<td style="text-align: left;">rhcos-4.12.3-s390x-live-initramfs.s390x.img</td>
</tr>
<tr>
<td style="text-align: left;"><strong>rhcos_live_rootfs</strong></td>
<td style="text-align: left;">CoreOS rootfs to be used for the bootstrap, control and compute nodes.</td>
<td style="text-align: left;">rhcos-4.12.3-s390x-live-rootfs.s390x.img</td>
</tr>
</tbody>
</table>
<h2 id="16-hypershift-optional">16 - Hypershift ( Optional )<a class="headerlink" href="#16-hypershift-optional" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>hypershift.kvm_host</strong></td>
<td style="text-align: left;">IPv4 address of KVM host for hypershift <br /> (kvm host where you want to run all oc commands and create VMs)</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.kvm_host_user</strong></td>
<td style="text-align: left;">User for KVM host</td>
<td style="text-align: left;">root</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_hypershift</strong></td>
<td style="text-align: left;">IPv4 address for bastion of Hosted Cluster</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_hypershift_user</strong></td>
<td style="text-align: left;">User for bastion of Hosted Cluster</td>
<td style="text-align: left;">root</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.create_bastion</strong></td>
<td style="text-align: left;">true or false - create bastion with the provided IP (hypershift.bastion_hypershift)</td>
<td style="text-align: left;">true</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.networking_device</strong></td>
<td style="text-align: left;">The network interface card from Linux's perspective. <br /> Usually enc and then a number that comes from the dev_num of the network adapter.</td>
<td style="text-align: left;">enc1100</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.gateway</strong></td>
<td style="text-align: left;">IPv4 Address for gateway from where the kvm_host and bastion are reachable  <br /> This for adding ip route from kvm_host to bastion through gateway</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.interface</strong></td>
<td style="text-align: left;">Interface for bastion</td>
<td style="text-align: left;">enc1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.hostname</strong></td>
<td style="text-align: left;">Hostname for bastion</td>
<td style="text-align: left;">bastion</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.base_domain</strong></td>
<td style="text-align: left;">DNS base domain for the bastion.</td>
<td style="text-align: left;">ihost.com</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.os_variant</strong></td>
<td style="text-align: left;">rhel os variant for creating bastion</td>
<td style="text-align: left;">8.7</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.nameserver</strong></td>
<td style="text-align: left;">Nameserver for creating bastion</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.gateway</strong></td>
<td style="text-align: left;">Gateway IP for creating bastion <br /> This is how it well be used ip=<ipv4 address>::<nameserver>:<subnet mask></td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.bastion_parms.subnet_mask</strong></td>
<td style="text-align: left;">IPv4 address of subnetmask</td>
<td style="text-align: left;">255.255.255.0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.mgmt_cluster_nameserver</strong></td>
<td style="text-align: left;">IP Address of Nameserver of Management Cluster</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.oc_url</strong></td>
<td style="text-align: left;">URL for OC Client that you want to install on the host</td>
<td style="text-align: left;">https://...<br /> ..openshift-client-linux-4.13.0-ec.4.tar.gz</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.clusters_namespace</strong></td>
<td style="text-align: left;">Namespace for Creating Hosted Control Plane</td>
<td style="text-align: left;">clusters</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.hosted_cluster_name</strong></td>
<td style="text-align: left;">Name for the Hosted Cluster</td>
<td style="text-align: left;">hosted0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.basedomain</strong></td>
<td style="text-align: left;">Base domain for Hosted Cluster</td>
<td style="text-align: left;">example.com</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.pull_secret_file</strong></td>
<td style="text-align: left;">Path for the pull secret <br /> No need to change this as we are copying the pullsecret to same file <br /> /root/ansible_workdir/auth_file</td>
<td style="text-align: left;">/root/ansible_workdir/auth_file</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.ocp_release</strong></td>
<td style="text-align: left;">OCP Release version for Hosted Control Cluster and Nodepool</td>
<td style="text-align: left;">4.13.0-rc.4-multi</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.machine_cidr</strong></td>
<td style="text-align: left;">Machines CIDR for Hosted Cluster</td>
<td style="text-align: left;">192.168.122.0/24</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.arch</strong></td>
<td style="text-align: left;">Architecture for InfraEnv and AgentServiceConfig"</td>
<td style="text-align: left;">s390x</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.hcp.pull_secret</strong></td>
<td style="text-align: left;">Pull Secret of Management Cluster <br /> Make sure to enclose pull_secret in 'single quotes'</td>
<td style="text-align: left;">'{"auths":{"cloud.openshift<br />.com":{"auth":"b3Blb<br />...<br />4yQQ==","email":"redhat.<br />user@gmail.com"}}}'</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.mce.version</strong></td>
<td style="text-align: left;">version for multicluster-engine Operator</td>
<td style="text-align: left;">2.4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.mce.instance_name</strong></td>
<td style="text-align: left;">name of the MultiClusterEngine instance</td>
<td style="text-align: left;">engine</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.mce.delete</strong></td>
<td style="text-align: left;">true or false - deletes mce and related resources while running deletion playbook</td>
<td style="text-align: left;">true</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.url_for_ocp_release_file</strong></td>
<td style="text-align: left;">Add URL for OCP release.txt File</td>
<td style="text-align: left;">https://... <br /> ..../release.txt</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.db_volume_size</strong></td>
<td style="text-align: left;">DatabaseStorage Volume Size</td>
<td style="text-align: left;">10Gi</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.fs_volume_size</strong></td>
<td style="text-align: left;">FileSystem Storage Volume Size</td>
<td style="text-align: left;">10Gi</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.ocp_version</strong></td>
<td style="text-align: left;">OCP Version for AgentServiceConfig</td>
<td style="text-align: left;">4.13.0-ec.4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.iso_url</strong></td>
<td style="text-align: left;">Give URL for ISO image</td>
<td style="text-align: left;">https://... <br /> ...s390x-live.s390x.iso</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.root_fs_url</strong></td>
<td style="text-align: left;">Give URL for rootfs image</td>
<td style="text-align: left;">https://... <br /> ... live-rootfs.s390x.img</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.asc.mce_namespace</strong></td>
<td style="text-align: left;">Namespace where your Multicluster Engine Operator is installed. <br /> Recommended Namespace for MCE is 'multicluster-engine'. <br /> Change this only if MCE is installed in other namespace.</td>
<td style="text-align: left;">multicluster-engine</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.static_ip_parms.static_ip</strong></td>
<td style="text-align: left;">true or false - use static IPs for agents using NMState</td>
<td style="text-align: left;">true</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.static_ip_parms.ip</strong></td>
<td style="text-align: left;">List of IP addresses for agents</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.static_ip_parms.interface</strong></td>
<td style="text-align: left;">Interface for agents for configuring NMStateConfig</td>
<td style="text-align: left;">eth0</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.agents_count</strong></td>
<td style="text-align: left;">Number of agents for the hosted cluster <br /> The same number of compute nodes will be attached to Hosted Cotrol Plane</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.agent_mac_addr</strong></td>
<td style="text-align: left;">List of macaddresses for the agents. <br /> Configure in DHCP if you are using dynamic IPs for Agents.</td>
<td style="text-align: left;">- 52:54:00:ba:d3:f7</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.disk_size</strong></td>
<td style="text-align: left;">Disk size for agents</td>
<td style="text-align: left;">100G</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.ram</strong></td>
<td style="text-align: left;">RAM for agents</td>
<td style="text-align: left;">16384</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.vcpus</strong></td>
<td style="text-align: left;">vCPUs for agents</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>hypershift.agents_parms.nameserver</strong></td>
<td style="text-align: left;">Nameserver to be used for agents</td>
<td style="text-align: left;">192.168.10.1</td>
</tr>
</tbody>
</table>
<h2 id="17-optional-create-compute-node-in-a-day-2-operation">17 - (Optional) Create compute node in a day-2 operation<a class="headerlink" href="#17-optional-create-compute-node-in-a-day-2-operation" title="Permanent link">#</a></h2>
<table>
<thead>
<tr>
<th style="text-align: left;"><strong>Variable Name</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Example</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>day2_compute_node.vm_name</strong></td>
<td style="text-align: left;">Name of the compute node VM.</td>
<td style="text-align: left;">compute-4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>day2_compute_node.vm_hostname</strong></td>
<td style="text-align: left;">Hostnames for compute node.</td>
<td style="text-align: left;">compute-4</td>
</tr>
<tr>
<td style="text-align: left;"><strong>day2_compute_node.vm_vm_ip</strong></td>
<td style="text-align: left;">IPv4 address of the compute node.</td>
<td style="text-align: left;">192.168.10.99</td>
</tr>
<tr>
<td style="text-align: left;"><strong>day2_compute_node.hostname</strong></td>
<td style="text-align: left;">The hostname of the KVM host</td>
<td style="text-align: left;">kvm-host-01</td>
</tr>
<tr>
<td style="text-align: left;"><strong>day2_compute_node.host_arch</strong></td>
<td style="text-align: left;">KVM host architecture.</td>
<td style="text-align: left;">s390x</td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../get-info/" class="btn btn-neutral float-left" title="1 Get Info"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../set-variables-host-vars/" class="btn btn-neutral float-right" title="3 Set Variables (host_vars)">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>Copyright © 2022 IBM zSystems Washington Systems Center</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/IBM/Ansible-OpenShift-Provisioning" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../get-info/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../set-variables-host-vars/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
